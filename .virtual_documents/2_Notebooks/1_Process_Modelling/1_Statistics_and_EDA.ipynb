# import all necessary library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import SQLContext
from pyspark.sql.types import *


# reading dataset to dataframe
df = pd.read_excel(r"Dataset/Dresses_Attribute_Sales/Attribute DataSet.xlsx")
df.head()


df.to_csv(r"Dataset/Dresses_Attribute_Sales/Attribute DataSet.csv", index=False)


# read by spark

# initialize Spark
spark = SparkSession.builder.master("local").appName("Data description Spark").getOrCreate()

# reading dataset to dataframe
schema = StructType([
    StructField("Dress_ID", StringType(), True),
    StructField("Style", StringType(), True),
    StructField("Price", StringType(), True),
    StructField("Rating", FloatType(), True),
    StructField("Size", StringType(), True),
    StructField("Season", StringType(), True),
    StructField("NeckLine", StringType(), True),
    StructField("SleeveLength", StringType(), True),
    StructField("waiseline", StringType(), True),
    StructField("Material", StringType(), True),
    StructField("FabricType", StringType(), True),
    StructField("Decoration", StringType(), True),
    StructField("Pattern Type", StringType(), True),
    StructField("Recommendation", IntegerType(), True)])

df_spark = spark.read \
        .schema(schema) \
        .format("com.databricks.spark.csv") \
        .option("header", "true") \
        .load(r"Dataset/Dresses_Attribute_Sales/Attribute DataSet.csv")

df_spark.show(5)


data = pd.read_csv(r'Datasets/data_pre.csv')


import os, math, subprocess
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.metrics import roc_auc_score
from itables import init_notebook_mode,show
init_notebook_mode(all_interactive=False)

# some settings for displaying Pandas results
# pd.set_option('display.width', 2000)
# pd.set_option('display.max_rows', 500)
# pd.set_option('display.max_columns', 500)
# pd.set_option('display.precision', 4)
# pd.set_option('display.max_colwidth', -1)


class ExplorerDF:

    def __init__(self,):
        pass

    def get_pct_outliers_rarelabels(sr, n_dist = None, params_outliers =(1.5,), params_rarelabels =(25, 2, 0.1, )):
        nuni = n_dist.loc[sr.name] if n_dist is not None else sr.nunique()
        is_discrete = False
        if pd.api.types.is_numeric_dtype(sr):
            if nuni > 10:
                iqr = sr.quantile(0.75) - sr.quantile(0.25)
                interval = (sr.quantile(0.25) - params_outliers[0]*iqr, sr.quantile(0.75) + params_outliers[0]*iqr)
                mask_sr = ~(sr >= interval[0]) & (sr <= interval[1])
                return mask_sr.mean()
            elif nuni > 2:
                is_discrete = True
            else:
                return 0
        if pd.api.types.is_string_dtype(sr) or is_discrete:
            if nuni > params_rarelabels[0]:
                return 0
            else:
                theshold = min(1/(nuni*params_rarelabels[1]), params_rarelabels[2])
                val_dom = sr.value_counts()/sr.count()
                total_pct_rarelabel = val_dom.loc[val_dom<theshold].sum()
                return total_pct_rarelabel
            
def get_auc(y, var, flexible_sign=True):
    """
    AUC the hien kha nang predictive cua model voi bien Y,
    do vay khi AUC(y, var) ~ 0.5 (random guess) the hien var 
    khong co kha nang giai thich bien Y
    """
    try:  # numeric data
        df = pd.concat([y, var], axis = 1).dropna(subset=[var.name])
        var_ = df.iloc[:,1]
        y_ = df.iloc[:,0]
        # if label not only 1s/0s
        auc = roc_auc_score(y_score=var_, y_true=y_) if (var_.std() > 0) else 0.5
        # for evaluation only
        if (auc < 0.5) & (flexible_sign):
            auc = 1.0 - auc
        return auc
    except:  # categorical
            return np.nan

        def exploring(dataframe : pd.DataFrame, nsample=5, targetY = None):
            # check rows, cols
            total_records, total_columns = dataframe.shape[0], dataframe.shape[1]
            print(f"Total {total_records} records, {total_columns} columns")

            # check dtypes
            dty = dataframe.dtypes.rename("sub_type")

            # check distinct
            n_dist = dataframe.nunique().rename("n_distinct")
            pct_dist = (100*n_dist/total_records).round(2).rename("pct_distinct")

            # check missing
            n_miss = dataframe.isna().sum().rename("n_miss")
            pct_miss = (100*n_miss/total_records).round(2).rename("pct_miss")
            pct_coverage = (100 - pct_miss).rename("pct_coverage")

            # check negative
            n_neg = dataframe.applymap(lambda x: (x<0) if (isinstance(x, int) or isinstance(x, float))
                                        else False).sum().rename("n_negative")
            pct_neg = (100*n_neg/total_records).round(2).rename("pct_negative")

            # check zero
            n_zero = dataframe.applymap(lambda x: (x==0) if (isinstance(x, int) or isinstance(x, float))
                                    else False).sum().rename("n_zero")
        pct_zero = (100*n_zero/total_records).round(2).rename("pct_zero")

        # check outliers / rare labels
        pct_out = dataframe.apply(lambda x: ExplorerDF.get_pct_outliers_rarelabels(x, n_dist,))\
            .rename('pct_outliers_rarelabels').round(4)*100

        if targetY is not None:
            assert total_records == targetY.shape[0]
            # check correlation with targetY
            corr_Y = dataframe.corrwith(targetY, numeric_only = True)\
                .reindex(dataframe.columns).rename('corr_Y')
            # check auc between targetY and variable
            auc = dataframe.apply(lambda x: ExplorerDF.get_auc(targetY, x))\
            .rename('auc').round(4)
        else:
            corr_Y = auc = None

        # check description
        des_stat = dataframe.describe().transpose().reindex(dataframe.columns).fillna(0)

        # take samples
        pdf_sample = dataframe.sample(n=nsample).transpose()
        pdf_sample.columns = ["sample_{}".format(i+1) for i in range(nsample)]

        # output
        pdf_data = pd.concat([dty, n_dist, pct_dist, n_miss, pct_miss, pct_coverage, 
                            n_neg, pct_neg, n_zero, pct_zero, pct_out, corr_Y, 
                            auc, des_stat, pdf_sample], axis=1)

        return pdf_data

    def export_statistic_report(list_data, output_folder = None):
        """
        list_data: list of dataframes/path_csv_files that want to be explore 
        """
        output_dir =os.path.join(output_folder, 'data_statistic_report.xlsx') \
            if output_folder is not None else 'data_statistic_report.xlsx'
        with pd.ExcelWriter(output_dir) as writer:
            for i, ele in enumerate(list_data) :
                if (type(ele) == str) :
                    if os.path.exists(ele) :
                        name = os.path.splitext(os.path.basename(ele))[0]
                        df = pd.read_csv(ele)
                elif (type(ele) == pd.DataFrame):
                    name = f'DataFrame_{i}'
                    df = ele
                else :
                    print('Error type of element', i)
                    continue
                print(name, end = ": ")
                rp = ExplorerDF.exploring_stats(df)
                rp.reset_index().to_excel(writer, sheet_name=name, index = False)  


ExplorerDF.exploring(data, nsample=1)


from dataprep.eda import create_report

# show report
# create_report(df.convert_dtypes()).show_browser()
# create_report(data.convert_dtypes(convert_integer=False))


# dask with large data

from dataprep.eda import create_report
from dask import dataframe as dd
df_dask = dd.read_csv(r'Datasets/data_pre.csv')
create_report(df_dask).show_browser()


# use sqlite3
import sqlite3
conn = sqlite3.connect(r'sqlite:///E:/4. Score/LEAD.db')
df = pd.read_sql_query('select * from VMGLEAD_SYNC where RISK_SCORE > 400 ', conn)


# use ConnectorX
from dataprep.connector import read_sql
db = r'sqlite:///E:/4. Score/LEAD.db'
df1 = read_sql(db,'select * from VMGLEAD_SYNC where ID_CARD = \'030087000004\' ', partition_num = 8)
df2 = read_sql(db,'select * from VMGLEAD_SYNC where RISK_SCORE > 400 ', partition_num = 8,partition_on="RISK_SCORE")


# use create_db_report
from dataprep.eda import create_db_report
from dataprep.datasets import load_db
db_engine = load_db(r'sqlite:///E:/4. Score/LEAD.db')
create_db_report(db_engine)


from dataprep.eda import create_report

df_meta = pd.DataFrame({c: pd.Series(dtype=t) for c, t in 
                        [('report_date',int),('brandname',str), ('month',str) ,('template_id',str) ,('tenkh',str) ,
                         ('makh',str), ('kythongbao',str) ,('som3', float),('sotien', float),('diachi', str),]})

conn_Str = f'oracle+cx_oracle://score:Vmg102021@192.168.18.32:1521/?service_name=score'

df_oracle = dd.read_sql('BRANDNAME_WATER_BILL_OVERDUE', conn_Str ,index_col= 'user_id', meta = df_meta).reset_index()

create_report(df_oracle).show_browser()


# or
import sqlalchemy as sa
sqluri = f'oracle+cx_oracle://score:Vmg102021@192.168.18.32:1521/?service_name=score'
engine = sa.create_engine(sqluri)
df_meta = pd.DataFrame({c: pd.Series(dtype=t) for c, t in [('month',str) , ('makh',str),('sotien', float)]})
sa_meta = sa.MetaData()
sa_table = sa.Table("BRANDNAME_WATER_BILL_OVERDUE", sa_meta, autoload=True, autoload_with=engine)
sa_query = sa.select([sa_table]).where(sa_table.c.month == "202008")
sa_columns = [sa_table.c.month, sa_table.c.makh, sa_table.c.sotien]


df_oracle = dd.read_sql_table(sa_query, sqluri, index_col="user_id", columns=sa_columns, meta = df_meta).reset_index()
create_report(df_oracle).show_browser()


# or use connector_x
import dataprep.connector as cx
conn_str = "sqlite:///D:/INFO.db"
df = cx.read_sql(conn_str,'select * from DTTSD_TELCO_INFO limit 1000000',return_type = 'dask')


from dataprep import eda

## plot(df): plots the distribution/histogram of all column
eda.plot(data)

# plot(df, x): plots the distribution of column x in various ways and calculates column statistics
eda.plot(data, 'Age')

# plot(df, x, y): generates scatter plots depicting the relationship between columns x and y
eda.plot(data, 'Age','Salary')


# customized histogram plot

import plotly.graph_objects as go
from plotly.subplots import make_subplots

def bin_hist( df, listvar = None, compared_df = None, cols = 3):
    listvar = [listvar] if type(listvar)==str else (df.columns if listvar is None else listvar)
    cols = min(cols, len(listvar))
    rows = (len(listvar) // cols) + 1 if ((len(listvar) % cols) != 0) else (len(listvar) // cols)
    fig = make_subplots(rows = rows, cols = cols , subplot_titles=listvar)
    for i, var in enumerate(listvar):     
        fig.add_trace(go.Histogram( x = df[var], name="train", marker_color='#656FF4', bingroup=i, histnorm = 'percent')#, texttemplate="%{y}")
                      , row=i//cols + 1, col=i%cols + 1 )
        if compared_df is not None:
            fig.add_trace(go.Histogram( x = compared_df[var], name="test", marker_color='#F85341', bingroup=i, histnorm = 'percent')#, texttemplate="%{y}")
                          , row=i//cols + 1, col=i%cols + 1 )
        
    fig.update_layout(autosize = True,height=rows*400, barmode='group', bargap=0.2, bargroupgap=0.05, showlegend=False, yaxis_title="percentage")
    fig.update_xaxes(categoryorder='category ascending')
    return fig#.show(renderer="jpeg")

bin_hist(data, cols = 5).show(renderer="jpeg")


# kde plot

import matplotlib.pyplot as plt
import seaborn as sns

sns.set_theme(palette="rainbow", style="darkgrid")
get_ipython().run_line_magic("matplotlib", " inline")

def kdeplot( df1, var= None, df2 = None):
    var = var if (var is not None) else df1.select_dtypes(include=np.number).columns.tolist()
    cols = 1 if df2 is None else 2
    rows = len(var) if type(var)!=str else 1
    var = var if type(var)!=str else [var]
    plt.figure(figsize=(4*cols,4*rows))
    i = 1
    for r,x in enumerate(var):
        plt.subplot(rows,cols,i)
        sns.kdeplot(data = df1[x], color="blue")
        i+=1
        if df2 is not None:
            plt.subplot(rows,cols,i)
            sns.kdeplot(data = df2[x], color="red")
            i+=1

    plt.tight_layout()
    return plt

kdeplot(data)
    


# scatter plot
# plot(df, x, y): generates scatter plots depicting the relationship between columns x and y
eda.plot(data, 'Age','Salary')


# scatter plot

def corr_scat( var1, var2, data):
    plt.scatter(data[var1], data[var2])
    corr = np.round(np.corrcoef(data[var1], data[var2])[0, 1],2)
    plt.ylabel(var2)
    plt.xlabel(var1)
    plt.title(f'corr({var1}, {var2}) = {corr}')
    plt.show()

corr_scat('Age', 'Salary', data)


# use dataprep
from dataprep import eda

# correlation all columns with each other
eda.plot_correlation(data)

# correlation specific columns to each other
eda.plot_correlation(data, 'Age',)

# correlation 2 specific columns
eda.plot_correlation(data, 'Age', 'Salary',config ={'scatter.sample_size': 1000, 'height': 400, 'width': 400,} )


# customize function
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

def corr_heatmap(data, method = 'pearson'):
    corrmat = data.corr(method=method, numeric_only = True).round(2)
    fig = px.imshow(corrmat, 
                    text_auto=True,
                    zmin=-1,
                    zmax=1,
                    color_continuous_scale=px.colors.diverging.RdBu,
                    aspect="auto"
                   )
    return fig

corr_heatmap(data).show(renderer = 'jpeg')


def PairGridCorr(X, y = None, corr = 'pearson'):
    def corrdot(*args, **kwargs):
        corr_r = args[0].corr(args[1], corr)
        corr_text = f"{corr_r:2.2f}".replace("0.", ".")
        ax = plt.gca()
        ax.set_axis_off()
        marker_size = abs(corr_r) * 10000
        ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
                   vmin=-1, vmax=1, transform=ax.transAxes)
        font_size = abs(corr_r) * 50 + 5
        ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                    ha='center', va='center', fontsize=font_size)

    def annotate_colname(x, **kws):
        ax = plt.gca()
        ax.annotate(x.name, xy=(0.5, 0.9), xycoords=ax.transAxes, fontweight='bold')

    sns.set(style='white', font_scale=1.3)
    # sns.set_theme( style="whitegrid")
    g = sns.PairGrid(X, aspect=1.4, diag_sharey=False)
    g.map_lower(sns.scatterplot, hue = y )
    g.map_diag(sns.histplot,)
    g.map_diag(annotate_colname)
    g.map_upper(corrdot)
    return g

PairGridCorr(data, data['Purchased'])


import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy import stats
import numpy as np

def diagnostic_plots(sr, comp_sr = None, var_name = None, qqplot = True, dist = 'norm', sparams = ()):
    """
    dist : distribution of qqplot
    sparams : parameter to determine the distribution, if it has
    """
    var = var_name if var_name is not None else (sr.name if hasattr(sr, 'name') else 'variable')
    datas = [sr] if comp_sr is None else [sr, comp_sr]
    cols = 2 if qqplot else 1
    rows = 2 if (comp_sr is not None) else 1
    subplot_titles = [f"Histogram of {var}"]
    if qqplot:
        subplot_titles = subplot_titles + [f"Q-Q plot of {var}"]
    if comp_sr is not None:
        subplot_titles = [i + " (1st)" for i in subplot_titles] + [i + " (2nd)" for i in subplot_titles]
        
    fig = make_subplots(rows = rows, cols = cols , subplot_titles=subplot_titles)
    
    for i, data in enumerate(datas):
        fig.add_trace(go.Histogram(x = data), row= i+1, col=1)
        if qqplot:
            qq = stats.probplot(data, dist = dist, sparams = sparams)
            x = np.array([qq[0][0][0], qq[0][0][-1]])
            fig.add_trace(go.Scatter(x=qq[0][0], y=qq[0][1], mode='markers'), row= i+1, col=2)
            fig.add_trace(go.Scatter(x=x, y=qq[1][1] + qq[1][0]*x, mode='lines'), row= i+1, col=2)
            
    fig.update_layout(autosize = True,height=rows*400, showlegend=False)#, width=cols*350
    return fig


import numpy as np

# Generate 1000 samples from a normal distribution with mean 0 and standard deviation 1
samples = np.random.normal(0, 1, 1000)

# Add some noise to the samples
noise = np.random.normal(0, 0.2, 1000)
samples_with_noise = samples + noise


diagnostic_plots(samples_with_noise).show(renderer="jpeg")


import plotly.express as px

class MissingAnalysis:

    def __init__(self, data, features_name = None):
        self.data = data
        # self.data_processed = data
        self.features_name = list(data.columns) if (type(data) == pd.DataFrame) else \
                            (features_name if features_name is not None else 
                            (['feature_' + str(i) for i in range(data.shape[1])]))

    def summary(self, ft_names = None):
        ft_names = self.features_name if ft_names is None else ft_names
        return plot_missing(self.data[ft_names])


    def count_na(self, ft_names = None, view = False):
        ft_names = self.features_name if ft_names is None else ft_names
        df = self.data[ft_names]
        na_count = df.isna().sum().rename('n_miss')
        na_perc = df.isna().mean().rename('pct_miss')
        res = pd.concat([na_count, na_perc], axis = 1).sort_values('n_miss', ascending = False)
        if not view:
            return res
        else:
            res['pct_miss'] = res['pct_miss'].map(lambda i: "{:.2f}".format(i*100)+'%')
            return res.style.background_gradient(cmap="Pastel1_r", subset=['n_miss'])
            # return res


    def compare_hist(df1, df2, ft_names = None, col = 4):

        if type(ft_names) == str:
            fig = create_distplot([df1[ft_names].dropna(), df2[ft_names].dropna()],
            ['Before', 'After'], show_rug=True, bin_size = 30)
            fig.show(renderer="jpeg")
        else:
            if ft_names is None:
                ft_names = [i for i in df1.columns if i in df2.columns]
                ft_names = [i for i in ft_names if not 
                        (pd.api.types.is_string_dtype(df1[i]) and df1[i].nunique() > 15)] # remove the categorical variables with nunique > 15
                row = len(ft_names)//col + 1
                fig = make_subplots(rows = row, cols=col, subplot_titles = ft_names)
                for i, obj in enumerate(ft_names):
                    fig.add_trace(go.Histogram(x = df1[obj] , marker_color = '#F85341' ,name = 'Before', bingroup = i)
                    , row=i//col + 1, col=i%col + 1)
                    fig.add_trace(go.Histogram(x = df2[obj] , marker_color = '#656FF4', name = 'After', bingroup = i)
                    , row=i//col + 1, col=i%col + 1)
                fig.update_layout(height=row*350, width=col*350, showlegend=False, barmode='overlay', overwrite=True)
                fig.update_xaxes(categoryorder='category ascending')
                fig.show(renderer="jpeg")

    def compare_boxplot(sr1, sr2):
        ft_name = sr1.name
        df1 = sr1.to_frame('data')
        df1[ft_name] = 'Before'
        df2 = sr2.to_frame('data')
        df2[ft_name] = 'After'
        df = pd.concat([df1, df2], axis = 0)
        fig = px.histogram(df, x='data', color=ft_name, marginal="box", barmode = 'overlay',title = ft_name)
        fig.update_xaxes(categoryorder='category ascending')
        fig.show(renderer="jpeg")


MissingAnalysis.compare_boxplot(data['Salary'],data['Salary'].fillna(0))


# use dataprep
from dataprep import eda

# REPORT FOR MISSING DATA
# Stats: report
# Bar Chart: Stack Bar missing percentile
# Spectrum: area miss in array
# Heatmap: correlation missing among all variables
# Dendrogram: cutoff point of missing-rate
eda.plot_missing(data)

# REPORT MISSING for specific variable
eda.plot_missing(data, 'Age')

eda.plot_missing(data, 'Age','Salary')


# plot missing matrix

import missingno as msno
# check pattern of missingness
msno.matrix(data)


# Plot
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

def hist_boxsplot(var, df_raw, df_outliers_handled=None):
    df = df_raw[[var]].copy()
    df['name_df'] = 'df_raw'
    if df_outliers_handled is not None:
        df1 = df_outliers_handled[[var]].copy()
        df1['name_df'] = 'df_outliers_handled'
        df = pd.concat([df,df1])
    
    fig = px.histogram(df, x=var, color="name_df", marginal="box",  hover_data=df.columns)
    fig.update_layout(barmode='overlay')
    fig.update_traces(opacity=0.75)
    fig.update_xaxes(categoryorder='category ascending')
    fig.show(renderer="jpeg")

def assign_inliers(sr, method = 'iqr', params = (1.5), retbin_inlier = False):
    """
    method is in ('std', 'iqr', 'quantile','arbitrary')
    params is parameter for each method:
    - for 'std' method: params = (multiple of std for inliers)
    - for 'iqr' method: params = (multiple of iqr for inliers)
    - for 'quantile' method: params = (lower quantile for inliers , upper quantile for inliers )
    - for 'arbitrary' method: params = (lowest value for inliers , largest value for inliers )
    retbin_inlier = True if return interval of inliers
    """
    if type(params) is not tuple:
        params = (params,)
        
    if method == 'std':
        interval = (sr.mean() - params[0]*sr.std(), sr.mean() + params[0]*sr.std())
    elif method == 'iqr':
        iqr = sr.quantile(0.75) - sr.quantile(0.25)
        interval = (sr.quantile(0.25) - params[0]*iqr, sr.quantile(0.75) + params[0]*iqr)
    elif method == 'quantile':
        interval = (sr.quantile(params[0]) , sr.quantile(params[1]))
    elif method == 'arbitrary':
        interval = (params[0] , params[1])
    res = (sr >= interval[0]) & (sr <= interval[1])
    
    if retbin_inlier:
        res = (res, interval)
    return res

def multivar_boxsplot( df_raw, listvar = None, compared_df = None, cols = 4):
    listvar = df_raw.select_dtypes(include=np.number).columns if listvar is None else listvar
    cols = min(cols, len(listvar))
    rows = (len(listvar) // cols) + 1 if ((len(listvar) % cols) != 0) else (len(listvar) // cols)
    subplot_titles = []
    for var in listvar:
        pct_outlier = (~assign_inliers(df_raw[var])).mean()
        subplot_titles.append('{} ({:.2f}%)'.format(var, pct_outlier*100))
    fig = make_subplots(rows = rows, cols = cols , subplot_titles=subplot_titles)
    for i, var in enumerate(listvar):
        fig.add_trace(go.Box(y = df_raw[var], name= 'before', boxmean='sd')
                    , row=i//cols + 1, col=i%cols + 1 )
        if compared_df is not None: 
            fig.add_trace(go.Box(y = compared_df[var], name= 'after', boxmean='sd')
                        , row=i//cols + 1, col=i%cols + 1 )
    fig.update_layout(autosize = True,height=rows*400, showlegend=False)
    return fig


multivar_boxsplot(data, cols = 5).show(renderer = 'jpeg')


# customized histogram plot for rare labels

import plotly.graph_objects as go
from plotly.subplots import make_subplots

def bin_hist( df, listvar = None, compared_df = None, cols = 3):
    listvar = [listvar] if type(listvar)==str else (df.columns if listvar is None else listvar)
    cols = min(cols, len(listvar))
    rows = (len(listvar) // cols) + 1 if ((len(listvar) % cols) != 0) else (len(listvar) // cols)
    fig = make_subplots(rows = rows, cols = cols , subplot_titles=listvar)
    for i, var in enumerate(listvar):     
        fig.add_trace(go.Histogram( x = df[var], name="train", marker_color='#656FF4', bingroup=i, histnorm = 'percent')#, texttemplate="%{y}")
                      , row=i//cols + 1, col=i%cols + 1 )
        if compared_df is not None:
            fig.add_trace(go.Histogram( x = compared_df[var], name="test", marker_color='#F85341', bingroup=i, histnorm = 'percent')#, texttemplate="%{y}")
                          , row=i//cols + 1, col=i%cols + 1 )
        
    fig.update_layout(autosize = True,height=rows*400, barmode='group', bargap=0.2, bargroupgap=0.05, showlegend=False, yaxis_title="percentage")
    fig.update_xaxes(categoryorder='category ascending')
    return fig#.show(renderer="jpeg")

bin_hist(data, cols = 5).show(renderer="jpeg")


# use dataprep

from dataprep import eda
from sklearn.model_selection import train_test_split

df_train, df_test = train_test_split(data, train_size = 0.7)

eda.plot_diff([df_train, df_test])


# Nomial
df_nomial = df[["Style", "NeckLine", "Material", "Pattern Type"]]
print(df_nomial.describe())
df_nomial.head(5)


df_spark[["Style", "NeckLine", "Material", "Pattern Type"]].describe().show()


# Numbers
df_numbers = df[["Dress_ID"]]
print(df_numbers.describe())
df_numbers.head(5)


# Ordinal
df_ordinal = df[["Size",'Price']]
print(df_ordinal.describe())
df_ordinal.head(5)


# ratio
df_ratio = df[["Rating"]]
print(df_ratio.describe())
df_ratio.head(5)


mean_rating = df_ratio["Rating"].mean()
median_rating = df_ratio["Rating"].median()
mode_rating = df_ratio["Rating"].mode().get(0)
print("Mean rating:", mean_rating)
print("Median rating:", median_rating)
print("Mode rating:", mode_rating)


# mean
mean_rating = df_spark.agg(F.mean(df_spark.Rating)).first()[0]

# median
df_spark.createOrReplaceTempView("df_spark")
median_rating = spark.sql("""
    SELECT percentile(Rating, 0.5) AS median_rating 
    FROM df_spark
""").first()["median_rating"]

print( "Mean rating:", mean_rating)
print( "Median rating:", median_rating)


# range of rating
min_rating = df_ratio["Rating"].min()
max_rating = df_ratio["Rating"].max()
range_rating = max_rating - min_rating
print( "Min rating:", min_rating)
print( "Max rating:", max_rating)
print( "Rating range:", range_rating)


# variance of rating
var_rating = df_ratio["Rating"].var()
print( "Rating variance:", var_rating)


# standard deviation of rating
std_rating = df_ratio["Rating"].std()
print( "Rating standard deviation:", std_rating)


# import libraries
import math
import numpy as np
import scipy.stats


# lấy mẫu 100 người
n = 100

# ủng hộ ứng viên A là 55%
candidate_A_prop = 0.55

# ủng hộ ứng viên B là 45%
candidate_B_prop = 0.45

# tính Zscore cho ứng viên A với độ tin cậy là 95%
qnorm = scipy.stats.norm.ppf(0.975)
print( "Z-score:", qnorm)

# calculate p hat
p = candidate_A_prop
std_err_prop = math.sqrt(p * (1 - p) / n)
print("Độ lỗi chuẩn cho tỉ lệ ủng hộ ứng viên A:", std_err_prop)

# calculate standard error of mean
std_of_means = std_err_prop / math.sqrt(n)
print("Độ lỗi chuẩn cho tỉ lệ thu thập được:", std_of_means)

# calculate interval
upper_limit = candidate_A_prop + qnorm * std_of_means
lower_limit = candidate_A_prop - qnorm * std_of_means
print( "Tỉ lệ ủng hộ chặn trên:", upper_limit)
print("Tỉ lệ ủng hộ chặn dưới:", lower_limit)


# thống kê thời gian các lượt chạy
list_run_time = np.asarray([11.3, 10.3, 9.6, 9.1, 8.9, 8.9, 8.1, 7.5, 6.9])
sample_size = list_run_time.size
sample_mean = list_run_time.mean()
sample_std = list_run_time.std()
standard_err = sample_std / math.sqrt(sample_size)

print( "Total runners:", sample_size)
print( "Sample mean:", sample_mean)
print( "Sample standard deviation:", sample_std)
print( "Standard/Sampling error:", standard_err)

# calculate interval
qnorm = scipy.stats.norm.ppf(0.975)
print( "Z-score:", qnorm)
upper_limit = sample_mean + qnorm * standard_err
lower_limit = sample_mean - qnorm * standard_err
print( "Thời gian trung bình chặn trên:", upper_limit)
print( "Thời gian trung bình chặn dưới:", lower_limit)


n_boys, n_girls = 14, 36
n = n_boys + n_girls
alpha = 0.05
p = 0.51
p_value = scipy.stats.binom.pmf(n_girls, n, p)
print('p_value = ', p_value)

# reject H0 or not
if p_value < alpha:
    print( "Reject H0 => nữ bị gọi là do thiên vị")
else:
    print( "Accept H0 => nữ bị gọi là do tình cờ")


# sample
n = 500
p_hat = yes = 0.54
p_0 = 0.5
z_p = (p_hat - p_0)/(math.sqrt(p_0 * (1 - p_0) / n))

# one_tail test
z_cdf = scipy.stats.norm.cdf(z_p)
p_value = 1 - z_cdf

print('p_value = ', p_value)

# reject H0 or not
if p_value < alpha:
    print( "Reject H0 => ứng viên thắng cử")
else:
    print( "Accept H0 => ứng viên thua cử")


n = 100
mu = 20.15
avg_weight = 20.05
std_weight = 0.26
z = (avg_weight - mu)/(std_weight/math.sqrt(n))

#one-tail test
p_value = scipy.stats.norm.cdf(z)
print('p_value = ', p_value)

# reject H0 or not
if p_value < alpha:
    print( "Reject H0 => không ship hàng")
else:
    print( "Accept H0 => ship hàng")


# import libraries
from scipy import stats
import math
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# t_test cho từng giá trị n tăng dần, với độ tin cậy 5% (2-tail)
print( "n = 3:", stats.t.ppf(0.975, 2))
print( "n = 10:", stats.t.ppf(0.975, 9))
print( "n = 10:", stats.t.ppf(0.975, 19))
print( "\nn = 100:", stats.t.ppf(0.975, 99))
print( "\nn = 1000:", stats.t.ppf(0.975, 99))
print( "z-score:", stats.norm.ppf(0.975))


scores = np.array([10, 9, 8, 8.5, 6, 6.5, 7.25, 9.5, 7.25, 8.25])
n_size = len(scores)
dof = n-1
CI = 0.98 # confidence interval

sample_mean = scores.mean()
t_score = stats.t.ppf(CI, dof)
standard_err = scores.std() / math.sqrt(n)
ucl = sample_mean + t_score * standard_err
lcl = sample_mean - t_score * standard_err

print('sample mean = ', sample_mean)
print(f'Khoang tin cay {lcl} - {ucl}')


# 1. Chia quần thể thành experiment và control group
group_aspirin = 2219
group_control = 2035
aspirin_heart_attacks = 26
control_heart_attacks = 46

# 2. Tính các thông số
p_aspirin_attacks = aspirin_heart_attacks/group_aspirin
p_control_attacks = control_heart_attacks/group_control
SE = math.sqrt(abs(p_aspirin_attacks * (1-p_aspirin_attacks)/group_aspirin + p_control_attacks * (1-p_control_attacks)/group_control))

    # tính khoảng tin cậy
diff_from_sample = abs(p_aspirin_attacks - p_control_attacks)
critical_value = stats.norm.ppf(0.975)
ucl = diff_from_sample + critical_value * SE
lcl = diff_from_sample - critical_value * SE

print( "Tỉ lệ phần trăm bị đau tim khi sử dụng aspirin:", p_aspirin_attacks )
print( "Tỉ lệ phần trăm bị đau tim khi không sử dụng aspirin:", p_control_attacks )
print( "Standard error giữa hai nhóm lấy mẫu:", SE)
print( "Differences from samples:", diff_from_sample)
print( "UCL:", ucl)
print( "LCL:", lcl)


# 3. Thực hiện kiểm định giả thuyết
# H0: cho rằng tỉ lệ mắc bệnh tim không thay đỗi. Nghĩa là p1 - p0 = 0
# H1: cho rằng tỉ lệ mắc bệnh tim sẽ giảm. Nghĩa là p1 - p0 != 0
# Chọn significant level (alpha, mức ý nghĩa) = 5%
# Ở đây là population nên ta sẽ chọn Z-statistic để test


z = diff_from_sample / SE
print( "Z-score:", z)
print( "Critical value:", critical_value)

if z > critical_value:
    print( "Reject H0 => Sử dụng aspirin làm giảm nguy cơ bị đau tim")
else:
    print( "Reject Ha => Sử dụng aspirin không làm giảm nguy cơ bị đau tim")


# Kết quả làm kiểm tra toán từ 2 nhóm
# A: học online
# B: học sách giáo khoa
# 30 câu trắc nghiệm, mỗi câu 1 điểm

def do_the_test():
    n = 100
    
    # cho từng nhóm làm kiểm tra
    group_A_scores = np.asarray([np.random.randint(1, 30) for p in range(0, n)])
    group_B_scores = np.asarray([np.random.randint(1, 30) for p in range(0, n)])

    # tính số điểm median của hai nhóm
    group_A_median_scores = np.median(group_A_scores)
    group_B_median_scores = np.median(group_B_scores)
    
    # so sánh sự khác biệt điểm số
    diff_AB = group_A_median_scores - group_B_median_scores
    return diff_AB

# Thực hiện kiểm tra nhiều lần
median_diffs = []
for i in range(0, 1000):
    median_diffs.append(do_the_test())

# biểu diễn bằng histogram    
plt.hist(median_diffs)
plt.xlabel("Med diff")
plt.ylabel("Frequency")
plt.show()


# 1. Chia quần thể thành digital và text group
n_digital_group = 130
n_text_group = 80

digital_mean = 15.8
text_mean = 14.5

digital_std = 2.8
text_std = 2.6

diff_of_mean = digital_mean - text_mean
print( "Difference mean scores:", diff_of_mean)


# 2. Chuẩn bị các thông số

# tính standard error
SE = math.sqrt(digital_std**2 / n_digital_group + text_std**2 / n_text_group)
print( "Standard error giữa hai nhóm lấy mẫu:", SE)

# tính khoảng tin cậy
critical_value = stats.norm.ppf(0.975)
ucl = diff_of_mean + critical_value * SE
lcl = diff_of_mean - critical_value * SE
print( "UCL:", ucl)
print( "LCL:", lcl)


# 3. Thực hiện kiểm định giả thuyết
# H0: cho rằng học online không có gì khác học sách giáo khoa. Nghĩa là mean1 - mean2 = 0
# H1: cho rằng học online tốt hơn học sách giáo khoa. Nghĩa là mean1 - mean2 > 0
# Chọn significant level (alpha, mức ý nghĩa) = 1%
# Ở đây là population nên ta sẽ chọn Z-statistic để test


z = diff_of_mean / SE
print( "Z-score:", z)

critical_value = stats.norm.ppf(0.99)
print( "Critical value:", critical_value)

if z > critical_value:
    print( "Reject H0 => học online tốt hơn học sách giáo khoa")
else:
    print( "Reject Ha => học online không có gì khác học sách giáo khoa")


# số lượt view trong tuần phía Facebook
facebook_page_view_report = np.asarray([30, 50, 60, 60])

# số lượt view trong tuần phía Google
google_page_view_report = np.asarray([45, 48, 55, 52])

# calculate Chi-square
diff_observed = (google_page_view_report - facebook_page_view_report)**2
facebook_inverse = 1.0 / facebook_page_view_report
X2 = np.inner(diff_observed, facebook_inverse)
print( "Chi-square:", X2)

# compare with critical value
k = facebook_page_view_report.size
dof = k - 1
critical_value = stats.chi2.ppf(0.95, dof)
print( "Critical value:", critical_value)

if X2 > critical_value:
    print( "Reject H0 => hai báo cáo là khác nhau")
else:
    print( "Reject Ha => hai báo cáo là như nhau")


film_A = np.asarray([5, 3, 5, 3])
film_B = np.asarray([8, 3, 4, 5])
film_C = np.asarray([5, 6, 5, 8])
film_D = np.asarray([4, 6, 8, 2])

# SST: Total sum of squares
mean_A = film_A.mean()
mean_B = film_B.mean()
mean_C = film_C.mean()
mean_D = film_D.mean()

mean_collect = np.asarray([mean_A, mean_B, mean_C, mean_D])
grand_mean = mean_collect.mean()
print( "Grand mean:", grand_mean)

ss_A = np.inner(film_A - grand_mean, film_A - grand_mean)
ss_B = np.inner(film_B - grand_mean, film_B - grand_mean)
ss_C = np.inner(film_C - grand_mean, film_C - grand_mean)
ss_D = np.inner(film_D - grand_mean, film_D - grand_mean)
SST = ss_A + ss_B + ss_C + ss_D
print( "Total sum of squares:", SST)


# SSW: Sum of squares within
sw_A = np.inner(film_A - mean_A, film_A - mean_A)
sw_B = np.inner(film_B - mean_B, film_B - mean_B)
sw_C = np.inner(film_C - mean_C, film_C - mean_C)
sw_D = np.inner(film_D - mean_D, film_D - mean_D)
SSW = sw_A + sw_B + sw_C + sw_D
print( "Sum of squares within:", SSW)


# SSB: Sum of squares between
n = film_A.size
SSB = n * np.inner(mean_collect - grand_mean, mean_collect - grand_mean)
print( "Sum of squares between:", SSB)


n = film_A.size + film_B.size + film_C.size + film_D.size
m = mean_collect.size
f_stat = (SSB / (m - 1)) / (SSW / (n - m))
print( "F statistics:", f_stat)

# get critical value
dfn = m - 1
dfd = n - m
critical_value = stats.f.ppf(0.95, dfn, dfd)
print( "Critical value:", critical_value)

if f_stat > critical_value:
    print("Reject H0 => đánh giá các bộ phim là khác nhau")
else:
    print("Reject Ha => đánh giá các bộ phim là như nhau")
