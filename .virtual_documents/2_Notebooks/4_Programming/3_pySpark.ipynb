# find the env spark
import findspark
findspark.init()

import pyspark

 # khởi tạo trình chạy pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Lesson1').getOrCreate()
spark 


# check cores
spark._jsc.sc().getExecutorMemoryStatus().keySet().size()


# create from row
from datetime import datetime, date
import pandas as pd
from pyspark.sql import Row

df = spark.createDataFrame([
    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),
    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),
    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))
])
df.show()


# create from tuple
df = spark.createDataFrame([
    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))
], schema='a long, b double, c string, d date, e timestamp')
df


# Create a PySpark DataFrame from an RDD consisting of a list of tuples.
rdd = spark.sparkContext.parallelize([
    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))
])
df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])
df


# from pandas.dataframe
pandas_df = pd.DataFrame({
    'a': [1, 2, 3],
    'b': [2., 3., 4.],
    'c': ['string1', 'string2', 'string3'],
    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],
    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]
})
df = spark.createDataFrame(pandas_df)
df


df.limit(3).show()


# show sample
df.show(1)


df.show(1, vertical=True)


# len df 
df.count()


spark.sql.repl.eagerEval.maxNumRows # setup maxrow to show
spark.conf.set('spark.sql.repl.eagerEval.enabled', True) # setup config to show in jupyter
df


# dtype
df.printSchema()


# convert pandas.df
df.toPandas()


# show columns
df.columns


import os
file = os.path.normpath(os.path.join(os.getcwd(),"..","0. Data",'datatest.csv')) ## ".." to get out folder
file


df = spark.read.csv(file,header=True)
df.show(1)


# csv
import os
# file = os.path.normpath(os.path.join(os.getcwd(),"..","2.Dask",'datatest','1989.csv')) ## ".." to get out folder
df = spark.read.csv(r"J:\My Drive\GitCode\My_learning\11. BigData\0. Data\datatest.csv",header=True)
df.show(1)


import myfunction as mf
mf.display(df.limit(4).toPandas())


# read parquet
df = spark.read.parquet('data.parquet') # read file parquet

# read many file parquet with the sample form
df_all = spark.read.parquet('data*.parquet')
df1_2 = spark.read.parquet('data1.parquet','data2.parquet')
df = spark.read.option('bathPath',path).parquet(path + '//data*.parquet') # set option
df = spark.read.parquet(folder1 +"//data*.parquet", folder2 +"//*") # set option


df.printSchema() # get dtype by SAMPLE of beginning datafile


df.describe()


df.schema['sepal_length']


from pyspark.sql.types import *

# set schema
data_schema = [
            # StructField('_c0',IntegerType()),
              StructField('sepal_length',FloatType()),
              StructField('sepal_width',FloatType()),
              # StructField('petal_length',FloatType()), # dont need to set all
              StructField('petal_width',FloatType()),
              StructField('species',StringType()),
              ]

final_struc = StructType(fields = data_schema)

df = spark.read.csv(file,header=True,schema = final_struc)

df.printSchema() # chỉ đọc những cột có định nghĩa trong final_struc schema


df.show(2)
df.printSchema()


# change the dtype columns
from pyspark.sql.types import *

df = df.withColumn('sepal_length',df['sepal_length'].cast(FloatType()) )\
        .withColumn('sepal_width',df['sepal_width'].cast(IntegerType()) )\
        .withColumn('sepal_length',to_date(df['sepal_length'],'yy.dd.mm') )\
        .withColumn('sepal_length',to_timestamp(df['petal_width'],'yy.dd.mm'))


df = spark.read.csv(r"J:\My Drive\GitCode\My_learning\11. BigData\0. Data\datatest.csv",header=True)


df.write.csv("/Volumes/GoogleDrive-106231888590528523181/My Drive/GitCode/My_learning/11. BigData/3. Spark/ab",'overwrite')


from py4j.java_gateway import java_import


java_import(spark._jvm,"org.apache.hadoop.fs.Path")


fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
file = fs.globStatus(spark._jvm.Path('write_test.csv/part*'))[0].getPath().getName()


df.write.parquet("namesAndFavColors.parquet")


from pyspark.sql import Column
from pyspark.sql.functions import upper


# select specific columns
df.select("a", "b", "c")#.describe().show()


df.select("*")


df.select('sepal_length','sepal_width','petal_length').summary('count','max','min').show()


df.select(df.c.isNull()).show()


# add new column
df.withColumn('upper_c', upper(df.c)).show()


#filter
df.filter(df.a == 1).show()


# csv
import os
# file = os.path.normpath(os.path.join(os.getcwd(),"..","2.Dask",'datatest','1989.csv')) ## ".." to get out folder
df = spark.read.csv(r"J:\My Drive\GitCode\My_learning\11. BigData\0. Data\datatest.csv",header=True)
df.show(1)


# rename columns
df.withColumnRenamed('sepal_length','col1').show(2)


# truncate to show all value in cell, not "..."
df.select(['sepal_length','sepal_width','petal_length']).show(3,truncate = False) 


# sort value
df.select(['sepal_length','sepal_width','petal_length']).orderBy(df['sepal_length']).show(5)
df.select(['sepal_length','sepal_width','petal_length']).orderBy(df['sepal_length'].desc()).show(5) # sort desc 
df.orderBy(df['sepal_length'].desc(),df['sepal_width']).show() # multi columns to sort


import os
# file = os.path.normpath(os.path.join(os.getcwd(),"..","2.Dask",'datatest','1989.csv')) ## ".." to get out folder
df = spark.read.csv(r"J:\My Drive\GitCode\My_learning\11. BigData\0. Data\datatest.csv",header=True)
df.show(1)


#filter
df.filter(df.sepal_length == 5.1).show()


# where va filter la nhu nhau 
df.select(['sepal_length','sepal_width','petal_length','species']).where(df.species.like("%eto%")).show(2)


df.select('species',df.species.substr(-4,3)).show(2) # string lấy substring từ -4, 3 ký tu


df[df.species.isin('setosa','versicolor')].show(2) # isin
df[df.species.startswith('s')].show(2) # startswith


df.select(df.columns[:3]).show(2)


# collect
a = df.where('sepal_length > 6').collect()
print(type(a[0]))
print(a[1][-1])


# slice
from pyspark.sql.functions import collect_list, slice
df2 = df.groupBy('species').agg(collect_list("sepal_length").alias("sepal_length_list"))
df2.show()
df2.select(slice(df2.sepal_length_list,2,2).alias('col1')).show() # slice(self, start, leght)


# use the APIs in a pandas Series within Python native function

import pandas
from pyspark.sql.functions import pandas_udf

@pandas_udf('long')
def pandas_plus_one(series: pd.Series) -> pd.Series:
    # Simply plus one by using pandas Series.
    return series + 1

df.select("*",pandas_plus_one('a').alias('age')).show() # select * rename columns as 'age'


from pyspark.sql.functions import expr
df.show(3)
df.withColumn('percent',expr("b/10*100")).show()


from pyspark.sql.functions import col as scol, sum as ssum, avg as savg, max as smax, count as scount


df = spark.createDataFrame([
    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],
    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],
    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])
df.show()


# groupby color with mean
df.groupby('color').avg().show()


df.groupBy("color","fruit").sum("v1","v2").show()


df.groupBy("color").agg(savg("v1").alias("avg_v1"), ssum("v2").alias("sum_v2"), smax("v2").alias("max_v2") ).show()


# tuong tu having sql
df.groupBy("color")\
.agg(savg("v1").alias("avg_v1"), 
     ssum("v2").alias("sum_v2"), 
     smax("v2").alias("max_v2") )\
.where(scol("max_v2") >= 50) \
.show(truncate=False)


# custom function for groupby
import pyspark.sql.functions as F
from pyspark.sql.types import FloatType

@F.udf(returnType=FloatType()) # define return float
def custom_func(x):
    return x[0] - x[-1]

df.groupby('color')\
.agg(F.collect_list("v1").alias("v1_groupby_color"))\
.withColumn('v1_custom', custom_func('v1_groupby_color'))\
.show()


# udf
from pyspark.sql.functions import udf

@udf(returnType = FloatType())
def square_float(x):
    return float(x**2)

df.select('v1',square_float('v1').alias('v1_squared')).show()


# multi columns in groupby
from pyspark.sql.functions import pandas_udf, PandasUDFType
from pyspark.sql.types import DoubleType
import numpy as np
@pandas_udf(DoubleType(), functionType=PandasUDFType.GROUPED_AGG)
def f(x, y):
    return np.mean([np.min(x), np.min(y)])

df.groupBy("color").agg(f("v1", "v2").alias("avg_min")).show()


@pandas_udf("color string, fruit string, v1 bigint, v2 bigint, v1_ float",functionType = PandasUDFType.GROUPED_MAP)  
def normalize(df):
    v1 = df.v1
    return df.assign(v1_=(v1 - v1.mean()) / v1.std())

df.groupby("color").apply(normalize).show()  


# set df table name
df.createOrReplaceTempView('table1')
spark.sql('select * from table1').limit(10).toPandas()


from pyspark.sql.types import sqlContext
sqlContext.registerDataFrameAsTable(df, "df_table")

median_rating = sqlContext.sql("""
    SELECT percentile(v2, 0.5) AS median_rating 
    FROM df_table
""").first()["median_rating"]

print("Median rating:", median_rating)


from pyspark.ml.feature import SQLTransformer
sqlTrans = SQLTransformer(statement='select * from __THIS__')
sqlTrans.transform(df).show(2)





# from pyspark.conf import SparkConf
# conf = SparkConf()  # create the configuration
# conf.set("spark.jars", r"C:\Spark_env\ojdbc11.jar")  # set the spark.jars
driver ="oracle.jdbc.driver.OracleDriver"# 'oracle.jdbc.driver.OracleDriver'
url = 'jdbc:oracle:thin:@192.168.18.32:1521/DTT'
user = 'datkt'
password = 'hct5Kg'
table = 'DTT_SD.IRIS'

df = spark.read \
    .format("jdbc") \
    .option("driver", driver) \
    .option("url", url) \
    .option("dbtable", table) \
    .option("user", user) \
    .option("password", password) \
    .load()



